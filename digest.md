# Weekly ToC Digest (week of 2026-02-10)

Prioritizing papers that closely align with classical and modern signal processing, particularly those integrating wavelets and neural networks. Relevance based on user's interests in harmonic analysis, wavelet theory, and modern deep learning for signal processing. Summarizing and prioritizing recent papers relevant to interests at the intersection of harmonic analysis, wavelet theory, and deep learning. Prioritized papers related to wavelet theory, multiresolution analysis, and deep learning. Focused on works involving signal processing, mathematical frameworks, and novel decomposition methods.

**Included:** 14 (score ≥ 0.35)  
**Scored:** 15 total items

---

## [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*arXiv Math*  
Score: **0.85**  
Published: 2026-02-10T05:00:00+00:00
Tags: wavelets, CNN, neural-networks

Proposes a backpropagation-free method using Extreme Learning Machines to address spectral bias in learning fine-scale details, relevant to wavelet-like decompositions in deep learning.

<details>
<summary>RSS summary</summary>

arXiv:2602.07603v1 Announce Type: cross Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with …

</details>

---

## [Fractional Filtering and Anomaly-Guided Diagnostics: The Local Damage Mode Extractor (LDME) for Early Gear Fault Detection](https://arxiv.org/abs/2602.07527)
*arXiv Signal Processing*  
Score: **0.85**  
Published: 2026-02-10T05:00:00+00:00
Tags: multiscale, filter-bank, time-frequency, methods

Leverages multiscale decomposition and fractional-domain enhancement, relevant to time-frequency and signal processing methods.

<details>
<summary>RSS summary</summary>

arXiv:2602.07527v1 Announce Type: new Abstract: Early and reliable detection of gear faults in complex drivetrain systems is critical for aviation safety and operational availability. We present the Local Damage Mode Extractor (LDME), a structured, physics-informed signal processing framework that combines dual-path denoising, multiscale decomposition, fractional-domain enhancement, and statistically principled anomaly scoring to produce interpretable condition indicators without supervision. LD…

</details>

---

## [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*arXiv Machine Learning*  
Score: **0.76**  
Published: 2026-02-10T05:00:00+00:00
Tags: GNN, methods, theory

Integrates finite-element computations directly into GNNs, aligning with modern signal processing in computational mechanics, an area of intersection with deep learning and harmonic analysis.

<details>
<summary>RSS summary</summary>

arXiv:2602.07364v1 Announce Type: new Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FE…

</details>

---

## [Data-Driven Discovery of Sign-Indefinite Artificial Viscosity for Linear Convection -- A Space-Time Reconvolution Perspective](https://arxiv.org/abs/2602.07733)
*arXiv Math*  
Score: **0.75**  
Published: 2026-02-10T05:00:00+00:00
Tags: time-frequency, methods

Uses data-driven methods for discovering artificial viscosity in reconvolution perspectives, potentially applicable to adaptive representations in signal processing.

<details>
<summary>RSS summary</summary>

arXiv:2602.07733v1 Announce Type: new Abstract: Artificial viscosity is traditionally interpreted as a positive, spatially acting regularization introduced to stabilize numerical discretizations of hyperbolic conservation laws. In this work, we report a data-driven discovery that motivates a reinterpretation of this classical view. We consider the linear convection equation discretized using an unstable FTCS scheme augmented with a learnable artificial viscosity. Using automatic differentiation …

</details>

---

## [Deep Energy Method with Large Language Model assistance: an open-source Streamlit-based platform for solving variational PDEs](https://arxiv.org/abs/2602.07838)
*arXiv Math*  
Score: **0.70**  
Published: 2026-02-10T05:00:00+00:00
Tags: methods, neural-networks

Involves using neural networks to solve PDEs, analogous to energy methods in classical mechanics and useful in bridging deep learning with traditional mathematical frameworks.

<details>
<summary>RSS summary</summary>

arXiv:2602.07838v1 Announce Type: new Abstract: Physics-informed neural networks (PINNs) in energy form, also known as the deep energy method (DEM), offer advantages over strong-form PINNs such as lower-order derivatives and fewer hyperparameters, yet dedicated and user-friendly software for energy-form PINNs remains scarce. To address this gap, we present \textbf{LM-DEM} (Large-Model-assisted Deep Energy Method), an open-source, Streamlit-based platform for solving variational partial different…

</details>

---

## [A Unifying Framework for Doubling Algorithms](https://arxiv.org/abs/2602.07250)
*arXiv Math*  
Score: **0.65**  
Published: 2026-02-10T05:00:00+00:00
Tags: methods, filter-bank

Focuses on efficient algorithms for matrix equations, which are applicable in signal processing contexts including filter design and adaptation.

<details>
<summary>RSS summary</summary>

arXiv:2602.07250v1 Announce Type: new Abstract: The existing doubling algorithms have been proven efficient for several important nonlinear matrix equations arising from real-world engineering applications. In a nutshell, the algorithms iteratively compute a basis matrix, in one of the two particular forms, for the eigenspace of some matrix pencil associated with its eigenvalues in certain complex region such as the left-half plane or the open unit disk, and their success critically depends on t…

</details>

---

## [Arithmetical enhancements of the Kogbetliantz method for the SVD of order two](https://arxiv.org/abs/2407.13116)
*arXiv Math*  
Score: **0.60**  
Published: 2026-02-10T05:00:00+00:00
Tags: theory, wavelets

Enhancements in SVD computations are directly relevant for applications in multiresolution analysis and wavelet transforms where SVD plays a critical role.

<details>
<summary>RSS summary</summary>

arXiv:2407.13116v3 Announce Type: replace Abstract: An enhanced Kogbetliantz method for the singular value decomposition (SVD) of general matrices of order two is proposed. The method consists of three phases: an almost exact prescaling, that can be beneficial to the LAPACK's xLASV2 routine for the SVD of upper triangular 2x2 matrices as well, a highly relatively accurate triangularization in the absence of underflows, and an alternative procedure for computing the SVD of triangular matrices, th…

</details>

---

## [Deep Reinforcement Learning for Interference Suppression in RIS-Aided Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2602.06982)
*arXiv Signal Processing*  
Score: **0.60**  
Published: 2026-02-10T05:00:00+00:00
Tags: deep learning, methods, spectrum

Focuses on spectrum sharing and interference suppression with deep learning, relevant to signal processing and wireless communications.

<details>
<summary>RSS summary</summary>

arXiv:2602.06982v1 Announce Type: new Abstract: Future 6G networks envision ubiquitous connectivity through space-air-ground integrated networks (SAGINs), where high-altitude platform stations (HAPSs) and satellites complement terrestrial systems to provide wide-area, low-latency coverage. However, the rapid growth of terrestrial devices intensifies spectrum sharing between terrestrial and non-terrestrial segments, resulting in severe cross-tier interference. In particular, frequency sharing bet…

</details>

---

## [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*arXiv Machine Learning*  
Score: **0.50**  
Published: 2026-02-10T05:00:00+00:00
Tags: LLM, time-series, methods

Presents a novel approach to modeling time-series data, which can be relevant for temporal signal processing connected to wavelet and deep learning interests.

<details>
<summary>RSS summary</summary>

arXiv:2602.07030v1 Announce Type: new Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Langu…

</details>

---

## [Behavior Score Prediction in Resting-State Functional MRI by Deep State Space Modeling](https://arxiv.org/abs/2602.07131)
*arXiv Signal Processing*  
Score: **0.40**  
Published: 2026-02-10T05:00:00+00:00
Tags: deep learning, methods

Uses deep modeling techniques for time-series analysis, applicable in contexts involving signal processing frameworks.

---

## [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*arXiv Machine Learning*  
Score: **0.40**  
Published: 2026-02-10T05:00:00+00:00
Tags: theory, optimization

Explores convex-like dynamics in deep learning, linking optimization with underlying mathematical properties pertinent to signal processing frameworks.

<details>
<summary>RSS summary</summary>

arXiv:2602.07145v1 Announce Type: new Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly beco…

</details>

---

## [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*arXiv Machine Learning*  
Score: **0.40**  
Published: 2026-02-10T05:00:00+00:00
Tags: methods, sparse

Introduces a sparse attention method using sketching techniques, which may relate to efficient signal representation.

<details>
<summary>RSS summary</summary>

arXiv:2602.07397v1 Announce Type: new Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&amp;Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&amp;Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layer…

</details>

---

## [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*arXiv Computer Vision*  
Score: **0.40**  
Published: 2026-02-10T05:00:00+00:00
Tags: sparse, methods

Utilizes sparse coding techniques, connecting to interests in sparse representations.

<details>
<summary>RSS summary</summary>

arXiv:2602.07311v1 Announce Type: new Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoenco…

</details>

---

## [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*arXiv Machine Learning*  
Score: **0.35**  
Published: 2026-02-10T05:00:00+00:00
Tags: CNN, methods

Focuses on the internal representations of neural policies which could connect to adaptive representations and learning mechanisms in signal processing.

<details>
<summary>RSS summary</summary>

arXiv:2602.07216v1 Announce Type: new Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and trai…

</details>

---
